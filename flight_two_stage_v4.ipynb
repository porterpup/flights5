{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyPuhefuE19q9t6etAgGhSyL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/porterpup/flights5/blob/main/flight_two_stage_v4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unmount drive"
      ],
      "metadata": {
        "id": "yaqP7evGLpWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Unmount Google Drive using the shell command\n",
        "print(\"Attempting to unmount Google Drive...\")\n",
        "# Use the fusermount command to unmount the drive\n",
        "!fusermount -uz /content/drive\n",
        "print(\"Google Drive unmount attempt complete.\")\n",
        "\n",
        "# You can optionally check if the mount point is empty after unmounting\n",
        "import os\n",
        "mountpoint = '/content/drive'\n",
        "# Check if the directory exists and is empty.\n",
        "# os.listdir will raise FileNotFoundError if the directory doesn't exist after unmounting,\n",
        "# which is a good indicator it was unmounted.\n",
        "if not os.path.exists(mountpoint) or not os.listdir(mountpoint):\n",
        "    print(f\"Google Drive successfully unmounted from {mountpoint}.\")\n",
        "else:\n",
        "    print(f\"Warning: Google Drive mount point {mountpoint} is not empty after unmount attempt.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QT9TbwkLoqz",
        "outputId": "3c7ed2c3-598a-49b5-8a05-d95c7c362cb7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to unmount Google Drive...\n",
            "Google Drive unmount attempt complete.\n",
            "Google Drive successfully unmounted from /content/drive.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount"
      ],
      "metadata": {
        "id": "LM4CVB-CL0dV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Check if the mountpoint exists and is not empty\n",
        "mountpoint = '/content/drive'\n",
        "\n",
        "# --- REMOVE this block as it is causing the error and is not necessary for mounting ---\n",
        "# if os.path.exists(mountpoint) and os.path.isdir(mountpoint) and os.listdir(mountpoint):\n",
        "#     print(f\"Removing existing files from {mountpoint}...\")\n",
        "#     for item in os.listdir(mountpoint):\n",
        "#         # Skip hidden files and directories (like .shortcut-targets-by-id)\n",
        "#         if item.startswith('.'):\n",
        "#             print(f\"Skipping hidden item: {item}\")\n",
        "#             continue\n",
        "\n",
        "#         item_path = os.path.join(mountpoint, item)\n",
        "#         if os.path.isfile(item_path):\n",
        "#             os.remove(item_path)\n",
        "#         elif os.path.isdir(item_path):\n",
        "#             shutil.rmtree(item_path)\n",
        "#     print(f\"Contents of {mountpoint} removed.\")\n",
        "# --- End of block to remove ---\n",
        "\n",
        "\n",
        "# Mount Google Drive - this handles the mounting process and should not require pre-cleaning\n",
        "print(f\"Attempting to mount Google Drive at {mountpoint}...\")\n",
        "drive.mount(mountpoint)\n",
        "print(\"Google Drive mount attempt complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zM5TZk66VTYq",
        "outputId": "d3f91fe9-3b6d-45ee-e021-cd2047ed6d37"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to mount Google Drive at /content/drive...\n",
            "Mounted at /content/drive\n",
            "Google Drive mount attempt complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 1 - Mount Drive - Only run once"
      ],
      "metadata": {
        "id": "TnIhb3zy719v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbXq0NL372jn",
        "outputId": "720bc95b-bfb3-44a8-84b9-fb68335a49a3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 2 - Installing packages"
      ],
      "metadata": {
        "id": "8A-_ka7J8UeB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas numpy scikit-learn lightgbm airportsdata\n",
        "!pip install git+https://github.com/meteostat/meteostat-python.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9RjcN3D8Xhq",
        "outputId": "da10a2b6-f2fa-45d5-9b28-094c986e2e21"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\n",
            "Requirement already satisfied: airportsdata in /usr/local/lib/python3.11/dist-packages (20250523)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Collecting git+https://github.com/meteostat/meteostat-python.git\n",
            "  Cloning https://github.com/meteostat/meteostat-python.git to /tmp/pip-req-build-5gy7uk74\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/meteostat/meteostat-python.git /tmp/pip-req-build-5gy7uk74\n",
            "  Resolved https://github.com/meteostat/meteostat-python.git to commit fc9f1ba5ff9b8f9f2a60e3ba8f3a724fe538c6b3\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas>=2 in /usr/local/lib/python3.11/dist-packages (from meteostat==1.7.1) (2.2.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from meteostat==1.7.1) (2025.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from meteostat==1.7.1) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2->meteostat==1.7.1) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2->meteostat==1.7.1) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2->meteostat==1.7.1) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 3 - Imports & Helper Functions"
      ],
      "metadata": {
        "id": "7_4Wa-DN8dc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, mean_absolute_error\n",
        "\n",
        "# Convert HHMM (e.g. 0530 ‚Üí 330 minutes) to ‚Äúminutes since midnight.‚Äù\n",
        "def hhmm_to_minutes(val):\n",
        "    try:\n",
        "        val = int(val)\n",
        "    except:\n",
        "        return np.nan\n",
        "    return (val // 100) * 60 + (val % 100)\n",
        "\n",
        "# Compute ARR_DELAY = max(actual_arrival_min ‚Äì scheduled_arrival_min, 0)\n",
        "def compute_arr_delay(row):\n",
        "    try:\n",
        "        arr_min = int(row['ARR_TIME'] // 100) * 60 + int(row['ARR_TIME'] % 100)\n",
        "        return max(arr_min - row['CRS_ARR_MIN'], 0)\n",
        "    except:\n",
        "        return np.nan\n"
      ],
      "metadata": {
        "id": "AThWfTb58hAi"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 4 - Congif & File Path"
      ],
      "metadata": {
        "id": "H4Du_0xa9I6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4 ‚Äì Write & verify cache files\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# 1) Define your cache directory (adjust the path to your Drive mount if needed)\n",
        "CACHE = '/content/drive/MyDrive/ColabFlightsData/cache'\n",
        "Path(CACHE).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 2) Define full paths\n",
        "FLIGHTS_CACHE   = os.path.join(CACHE, 'flights_v3.pkl')\n",
        "WEATHER_CACHE   = os.path.join(CACHE, 'weather_v3.pkl')\n",
        "AUGMENTED_CACHE = os.path.join(CACHE, 'augmented_v3.pkl')\n",
        "\n",
        "# 3) Write out your DataFrames\n",
        "#    (Make sure `flights`, `weather_df`, and `augmented` are already in memory)\n",
        "flights.to_pickle(FLIGHTS_CACHE)\n",
        "weather_df.to_pickle(WEATHER_CACHE)\n",
        "augmented.to_pickle(AUGMENTED_CACHE)\n",
        "\n",
        "# 4) Verify & report\n",
        "cache_files = {\n",
        "    'Flights cache':      FLIGHTS_CACHE,\n",
        "    'Weather cache':      WEATHER_CACHE,\n",
        "    'Augmented cache':    AUGMENTED_CACHE\n",
        "}\n",
        "\n",
        "for label, path in cache_files.items():\n",
        "    if os.path.exists(path):\n",
        "        size = os.path.getsize(path)\n",
        "        print(f\"‚úÖ {label} found at {path} ({size:,} bytes)\")\n",
        "    else:\n",
        "        print(f\"‚ùå {label} MISSING at {path}\")\n"
      ],
      "metadata": {
        "id": "eo9R4KPy9P6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ae64005-aecb-4052-c2fa-a8c6c0d68a57"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Flights cache found at /content/drive/MyDrive/ColabFlightsData/cache/flights_v3.pkl (31,286,975 bytes)\n",
            "‚úÖ Weather cache found at /content/drive/MyDrive/ColabFlightsData/cache/weather_v3.pkl (188,580 bytes)\n",
            "‚úÖ Augmented cache found at /content/drive/MyDrive/ColabFlightsData/cache/augmented_v3.pkl (554,790,008 bytes)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 5 - Build or load flights_v3.pkl"
      ],
      "metadata": {
        "id": "lQetwtWf9XLS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5 ‚Äì Build or Load ‚Äúflights_v3.pkl‚Äù (filter to ORIGIN == \"ATL\" and compute DEP_HOUR_BIN)\n",
        "\n",
        "# Ensure Google Drive is mounted - you might have already done this, but good to double-check\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True) # Use force_remount to ensure it's mounted\n",
        "\n",
        "# Verify the data folder exists after mounting\n",
        "import os\n",
        "if not os.path.exists(DATA_FOLDER):\n",
        "    raise FileNotFoundError(f\"Data folder not found after mounting: {DATA_FOLDER}. Please check the path and your Google Drive.\")\n",
        "\n",
        "if os.path.exists(FLIGHTS_CACHE):\n",
        "    flights = pd.read_pickle(FLIGHTS_CACHE)\n",
        "    flights.columns = flights.columns.str.upper()\n",
        "    print(\"Loaded cached flights_v3:\", flights.shape)\n",
        "else:\n",
        "    df_list = []\n",
        "    for path in MONTH_FILES:\n",
        "        # Keep the existing file existence check, but the check above is primary\n",
        "        if not os.path.exists(path):\n",
        "            raise FileNotFoundError(f\"Missing {path}. Please check if the file exists in your Google Drive at this location.\")\n",
        "        df = pd.read_csv(path)\n",
        "        df.columns = df.columns.str.upper()\n",
        "        df_list.append(df)\n",
        "    flights = pd.concat(df_list, ignore_index=True)\n",
        "    print(\"Combined flights shape:\", flights.shape)\n",
        "\n",
        "    # 1) Keep only UA, DL, AA\n",
        "    flights = flights[flights['OP_UNIQUE_CARRIER'].isin(CARRIERS)].copy()\n",
        "    print(\"After filtering carriers:\", flights.shape)\n",
        "\n",
        "    # 2) Filter to a single airport (ATL)\n",
        "    flights = flights[flights['ORIGIN'] == AIRPORT].copy()\n",
        "    print(f\"After filtering to {AIRPORT} only:\", flights.shape)\n",
        "\n",
        "    # 3) Parse FL_DATE ‚Üí datetime & derive DAY_OF_WEEK\n",
        "    flights['FL_DATE'] = pd.to_datetime(flights['FL_DATE'], errors='coerce')\n",
        "    flights = flights.dropna(subset=['FL_DATE']).copy()\n",
        "    flights['DAY_OF_WEEK'] = flights['FL_DATE'].dt.dayofweek + 1\n",
        "\n",
        "    # 4) Convert scheduled HHMM ‚Üí minutes\n",
        "    flights['CRS_DEP_MIN'] = flights['CRS_DEP_TIME'].apply(hhmm_to_minutes)\n",
        "    flights['CRS_ARR_MIN'] = flights['CRS_ARR_TIME'].apply(hhmm_to_minutes)\n",
        "    flights = flights.dropna(subset=['CRS_DEP_MIN', 'CRS_ARR_MIN']).copy()\n",
        "\n",
        "    # 5) Only keep rows with valid departure/arrival hours (0‚Äì23)\n",
        "    flights['DEP_HOUR'] = (flights['CRS_DEP_MIN'] // 60).astype(int)\n",
        "    flights['ARR_HOUR'] = (flights['CRS_ARR_MIN'] // 60).astype(int)\n",
        "    flights = flights[\n",
        "        flights['DEP_HOUR'].between(0, 23) &\n",
        "        flights['ARR_HOUR'].between(0, 23)\n",
        "    ].copy()\n",
        "\n",
        "    # 6) Create DEP_HOUR_BIN (early vs peak vs etc.)\n",
        "    def hour_bin(h):\n",
        "        if 4 <= h <= 7:\n",
        "            return 'pre_peak'\n",
        "        if 8 <= h <= 11:\n",
        "            return 'morning_bank'\n",
        "        if 12 <= h <= 15:\n",
        "            return 'midday'\n",
        "        if 16 <= h <= 19:\n",
        "            return 'afternoon'\n",
        "        if 20 <= h <= 23:\n",
        "            return 'evening'\n",
        "        return 'late_night'\n",
        "\n",
        "    flights['DEP_HOUR_BIN'] = flights['DEP_HOUR'].apply(hour_bin)\n",
        "\n",
        "    # 7) Numeric DISTANCE_GROUP\n",
        "    flights['DISTANCE_GROUP'] = pd.to_numeric(flights['DISTANCE_GROUP'], errors='coerce')\n",
        "    flights = flights.dropna(subset=['DISTANCE_GROUP']).copy()\n",
        "\n",
        "    # 8) Ensure DEP_DELAY_NEW is numeric & drop if missing\n",
        "    flights['DEP_DELAY_NEW'] = pd.to_numeric(flights.get('DEP_DELAY_NEW', np.nan), errors='coerce')\n",
        "    flights = flights.dropna(subset=['DEP_DELAY_NEW']).copy()\n",
        "\n",
        "    # 9) Save to cache\n",
        "    flights.to_pickle(FLIGHTS_CACHE)\n",
        "    print(\"Saved cached flights_v3:\", flights.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "jJPAFvWM9bR3",
        "outputId": "8b6ca7d2-3060-4dd1-ec36-9088a32a9c10"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Combined flights shape: (3461339, 24)\n",
            "After filtering carriers: (1345408, 24)\n",
            "After filtering to ATL only: (117540, 24)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "Cannot save file into a non-existent directory: '/content/drive/MyDrive/ColabFlightsData/cache'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-257083065>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;31m# 9) Save to cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mflights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLIGHTS_CACHE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Saved cached flights_v3:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                 )\n\u001b[0;32m--> 333\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_pickle\u001b[0;34m(self, path, compression, protocol, storage_options)\u001b[0m\n\u001b[1;32m   3163\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpickle\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3165\u001b[0;31m         to_pickle(\n\u001b[0m\u001b[1;32m   3166\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3167\u001b[0m             \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mto_pickle\u001b[0;34m(obj, filepath_or_buffer, compression, protocol, storage_options)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[0;31m# Only for write methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"r\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m         \u001b[0mcheck_parent_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mcheck_parent_directory\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    614\u001b[0m     \u001b[0mparent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 616\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mrf\"Cannot save file into a non-existent directory: '{parent}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Cannot save file into a non-existent directory: '/content/drive/MyDrive/ColabFlightsData/cache'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 6 Build or Load weather_v3.pkl"
      ],
      "metadata": {
        "id": "wtCKjbzZ9imj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(WEATHER_CACHE):\n",
        "    weather_df = pd.read_pickle(WEATHER_CACHE)\n",
        "    weather_df.columns = weather_df.columns.str.upper()\n",
        "    print(\"Loaded cached weather_v3:\", weather_df.shape)\n",
        "else:\n",
        "    from airportsdata import load\n",
        "    from meteostat import Stations, Hourly\n",
        "\n",
        "    # 1) Fetch full station inventory once\n",
        "    stations = Stations().fetch()\n",
        "    stations['icao'] = stations['icao'].str.upper()\n",
        "\n",
        "    # 2) Build IATA ‚Üí station_id map for US airports via ICAO = 'K' + IATA\n",
        "    iata_map = load('IATA')  # e.g. {'ATL':{'lat':33.6,'lon':-84.4,...}, ...}\n",
        "    station_lookup = {}\n",
        "    for iata_code, info in iata_map.items():\n",
        "        icao_code = f\"K{iata_code.upper()}\"\n",
        "        match = stations.index[stations['icao'] == icao_code]\n",
        "        if not match.empty:\n",
        "            station_lookup[iata_code] = match[0]\n",
        "\n",
        "    # 3) For any IATA not matched above (non-US), find nearest station in one vectorized pass\n",
        "    missing_iatas = [iata for iata in iata_map if iata not in station_lookup]\n",
        "    if missing_iatas:\n",
        "        st_lats = stations['latitude'].values\n",
        "        st_lons = stations['longitude'].values\n",
        "        st_idx  = stations.index.values\n",
        "        for iata_code in missing_iatas:\n",
        "            info = iata_map[iata_code]\n",
        "            lat0, lon0 = info['lat'], info['lon']\n",
        "            dx = st_lats - lat0\n",
        "            dy = st_lons - lon0\n",
        "            idx_min = st_idx[(dx*dx + dy*dy).argmin()]\n",
        "            station_lookup[iata_code] = idx_min\n",
        "\n",
        "    # 4) Build flights_subset = unique (FL_DATE, ORIGIN, DEP_HOUR)\n",
        "    flights_subset = flights[['FL_DATE','ORIGIN','DEP_HOUR']].drop_duplicates().reset_index(drop=True)\n",
        "    flights_subset['DATE'] = flights_subset['FL_DATE'].dt.date\n",
        "\n",
        "    # 5) Map ORIGIN ‚Üí STATION via station_lookup; drop if no station\n",
        "    flights_subset['STATION'] = flights_subset['ORIGIN'].map(station_lookup)\n",
        "    flights_subset = flights_subset.dropna(subset=['STATION']).reset_index(drop=True)\n",
        "\n",
        "    # 6) Add an index label so we can drop-duplicates after merge\n",
        "    flights_subset = flights_subset.reset_index().rename(columns={'index':'FL_IDX'})\n",
        "\n",
        "    weather_rows = []\n",
        "    for station_id in flights_subset['STATION'].unique():\n",
        "        sub = flights_subset[flights_subset['STATION'] == station_id]\n",
        "        start_date = sub['FL_DATE'].min().floor('D')\n",
        "        end_date   = sub['FL_DATE'].max().floor('D') + pd.Timedelta(days=1)\n",
        "\n",
        "        hourly_data = Hourly(station_id, start_date, end_date).fetch()\n",
        "        if hourly_data.empty:\n",
        "            continue\n",
        "\n",
        "        hourly_data = hourly_data.reset_index()  # ‚Äútime‚Äù becomes a column\n",
        "        hourly_data['DATE'] = hourly_data['time'].dt.date\n",
        "        hourly_data['HOUR'] = hourly_data['time'].dt.hour\n",
        "\n",
        "        merged = sub.merge(hourly_data, how='left', on='DATE')\n",
        "        merged['HOUR_DIFF'] = (merged['HOUR'] - merged['DEP_HOUR']).abs()\n",
        "        merged = merged[merged['HOUR_DIFF'] <= 1].copy()\n",
        "        if merged.empty:\n",
        "            continue\n",
        "\n",
        "        merged = ( merged\n",
        "                   .sort_values(['FL_IDX','HOUR_DIFF'])\n",
        "                   .drop_duplicates(subset='FL_IDX', keep='first') )\n",
        "\n",
        "        for _, row in merged.iterrows():\n",
        "            if any(pd.isna(row.get(f)) for f in ['temp','wspd','prcp','pres','rhum']):\n",
        "                continue\n",
        "            weather_rows.append({\n",
        "                'FL_DATE': pd.Timestamp(row['DATE']),\n",
        "                'ORIGIN':   row['ORIGIN'],\n",
        "                'DEP_HOUR': row['DEP_HOUR'],\n",
        "                'TEMP_C':   row['temp'],\n",
        "                'WIND_SPEED_KPH': row['wspd'],\n",
        "                'PRECIP_MM': row['prcp'],\n",
        "                'PRESSURE_HPA': row['pres'],\n",
        "                'HUMIDITY_PCT': row['rhum']\n",
        "            })\n",
        "\n",
        "    weather_df = pd.DataFrame(weather_rows)\n",
        "    weather_df.columns = weather_df.columns.str.upper()\n",
        "    weather_df.to_pickle(WEATHER_CACHE)\n",
        "    print(\"Saved cached weather_v3:\", weather_df.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krVWxWid9mhL",
        "outputId": "8a35d909-fc6d-4147-9808-3f49ffd6dcc3"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded cached weather_v3: (3222, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 7 - Build or load augumented_v3.pkl"
      ],
      "metadata": {
        "id": "ufB8GDim-uIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7 ‚Äì Full Feature Engineering & Cache (augmented_v3.pkl)\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
        "\n",
        "# 1) Paths & cache setup\n",
        "CACHE = '/content/drive/MyDrive/ColabFlightsData/cache'\n",
        "Path(CACHE).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "FLIGHTS_CACHE   = os.path.join(CACHE, 'flights_v3.pkl')\n",
        "WEATHER_CACHE   = os.path.join(CACHE, 'weather_v3.pkl')\n",
        "AUGMENT_CACHE   = os.path.join(CACHE, 'augmented_v3.pkl')\n",
        "\n",
        "# 2) Load base data\n",
        "flights    = pd.read_pickle(FLIGHTS_CACHE)\n",
        "weather_df = pd.read_pickle(WEATHER_CACHE)\n",
        "\n",
        "# 3) Ensure FL_DATE is datetime\n",
        "flights['FL_DATE'] = pd.to_datetime(flights['FL_DATE'], errors='coerce')\n",
        "\n",
        "# 4) Helper to convert HHMM ‚Üí minutes since midnight\n",
        "def hhmm_to_minutes(val):\n",
        "    try:\n",
        "        v = int(val)\n",
        "        return (v // 100) * 60 + (v % 100)\n",
        "    except:\n",
        "        return np.nan\n",
        "\n",
        "# 5) Compute DEP_HOUR, CRS times, ARR_DELAY\n",
        "flights['CRS_DEP_MIN'] = flights['CRS_DEP_TIME'].apply(hhmm_to_minutes)\n",
        "flights['DEP_HOUR']    = (flights['CRS_DEP_MIN'] // 60).astype(int)\n",
        "flights = flights[flights['DEP_HOUR'].between(0,23)].copy()\n",
        "\n",
        "flights['CRS_ARR_MIN'] = flights['CRS_ARR_TIME'].apply(hhmm_to_minutes)\n",
        "# If ARR_TIME is missing, use scheduled\n",
        "flights['ARR_TIME_USE'] = flights['ARR_TIME'].fillna(flights['CRS_ARR_TIME'])\n",
        "flights['ACT_ARR_MIN']  = flights['ARR_TIME_USE'].apply(hhmm_to_minutes)\n",
        "flights['ARR_DELAY']    = flights['ACT_ARR_MIN'] - flights['CRS_ARR_MIN']\n",
        "\n",
        "# 6) Merge with weather\n",
        "aug = flights.merge(\n",
        "    weather_df,\n",
        "    how='left',\n",
        "    on=['FL_DATE','ORIGIN','DEP_HOUR']\n",
        ")\n",
        "\n",
        "# 7) Drop rows missing core features\n",
        "core = ['DEP_DELAY_NEW','TEMP_C','WIND_SPEED_KPH','PRECIP_MM','PRESSURE_HPA','HUMIDITY_PCT','ARR_DELAY']\n",
        "aug = aug.dropna(subset=core).copy()\n",
        "\n",
        "# 8) Rolling 3-day & 14-day avg of ARR_DELAY per carrier+origin\n",
        "aug = aug.sort_values(['OP_UNIQUE_CARRIER','ORIGIN','FL_DATE'])\n",
        "aug['PAST3_AVG_DELAY']  = np.nan\n",
        "aug['PAST14_AVG_DELAY'] = np.nan\n",
        "\n",
        "for (carr, orig), grp in aug.groupby(['OP_UNIQUE_CARRIER','ORIGIN']):\n",
        "    grp = grp.sort_values('FL_DATE')\n",
        "    s = grp.set_index('FL_DATE')['ARR_DELAY'].shift()\n",
        "    roll3  = s.rolling('3d', closed='left').mean()\n",
        "    roll14 = s.rolling('14d', closed='left').mean()\n",
        "    aug.loc[grp.index, 'PAST3_AVG_DELAY']  = roll3.values\n",
        "    aug.loc[grp.index, 'PAST14_AVG_DELAY'] = roll14.values\n",
        "\n",
        "global_mean = aug['ARR_DELAY'].mean()\n",
        "aug['PAST3_AVG_DELAY'].fillna(global_mean, inplace=True)\n",
        "aug['PAST14_AVG_DELAY'].fillna(global_mean, inplace=True)\n",
        "\n",
        "# 9) Arrival time‚Äêof‚Äêday bucket\n",
        "aug['ARR_HOUR'] = (aug['CRS_ARR_MIN'] // 60).astype(int)\n",
        "def arr_bin(h):\n",
        "    if   0 <= h <= 5:   return 'late_night'\n",
        "    elif 6 <= h <= 11:  return 'morning'\n",
        "    elif 12 <= h <= 17: return 'afternoon'\n",
        "    else:               return 'evening'\n",
        "aug['ARR_DAY_PART'] = aug['ARR_HOUR'].apply(arr_bin)\n",
        "\n",
        "# 10) US federal holiday flag\n",
        "cal = USFederalHolidayCalendar()\n",
        "hols = cal.holidays(start=aug['FL_DATE'].min(), end=aug['FL_DATE'].max())\n",
        "aug['IS_HOLIDAY'] = aug['FL_DATE'].isin(hols).astype(int)\n",
        "\n",
        "# 11) 24-hour rolling late rate\n",
        "aug['FL_TS']   = pd.to_datetime(aug['FL_DATE'].dt.strftime('%Y-%m-%d') + ' ' +\n",
        "                                aug['DEP_HOUR'].astype(str) + ':00')\n",
        "aug['IS_LATE'] = (aug['ARR_DELAY'] > 0).astype(int)\n",
        "pct = aug.set_index('FL_TS')['IS_LATE'].rolling('24h').mean().fillna(aug['IS_LATE'].mean())\n",
        "aug['PCT_ALATLATE_24H'] = pct.values\n",
        "aug.drop(columns=['FL_TS','IS_LATE'], inplace=True)\n",
        "\n",
        "# 12) Season feature\n",
        "aug['MONTH'] = aug['FL_DATE'].dt.month\n",
        "def month_to_season(m):\n",
        "    if m in [12,1,2]: return 'winter'\n",
        "    if m in [3,4,5]:  return 'spring'\n",
        "    if m in [6,7,8]:  return 'summer'\n",
        "    return 'fall'\n",
        "aug['SEASON'] = aug['MONTH'].apply(month_to_season).astype('category')\n",
        "\n",
        "# 13) Severe‚Äêweather flag\n",
        "aug['IS_SEVERE_WEATHER'] = (\n",
        "    (aug['PRECIP_MM'] > 2.0) &\n",
        "    (aug['WIND_SPEED_KPH'] > 20.0)\n",
        ").astype(int)\n",
        "\n",
        "# 14) Hourly flight volume\n",
        "vol = flights.groupby(['FL_DATE','DEP_HOUR']).size().rename('HOURLY_VOL')\n",
        "aug = aug.merge(vol, how='left', on=['FL_DATE','DEP_HOUR']).fillna({'HOURLY_VOL':0})\n",
        "\n",
        "# 15) Ensure previous‚Äêleg delay\n",
        "aug['LATE_AIRCRAFT_DELAY'] = aug['LATE_AIRCRAFT_DELAY'].fillna(0.0)\n",
        "\n",
        "# 16) Cache the enriched dataset\n",
        "aug.to_pickle(AUGMENT_CACHE)\n",
        "print(\"üóÑÔ∏è  Built & cached augmented_v3:\", aug.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "ah1Hr6tO-1aR",
        "outputId": "dbd80594-641a-4175-cfde-a716414b45e3"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "index values must be monotonic",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-2559005230>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m                                 aug['DEP_HOUR'].astype(str) + ':00')\n\u001b[1;32m     88\u001b[0m \u001b[0maug\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'IS_LATE'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maug\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ARR_DELAY'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m \u001b[0mpct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maug\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'FL_TS'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'IS_LATE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrolling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'24h'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maug\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'IS_LATE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0maug\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'PCT_ALATLATE_24H'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0maug\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'FL_TS'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'IS_LATE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mrolling\u001b[0;34m(self, window, min_periods, center, win_type, on, axis, closed, step, method)\u001b[0m\n\u001b[1;32m  12578\u001b[0m             )\n\u001b[1;32m  12579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 12580\u001b[0;31m         return Rolling(\n\u001b[0m\u001b[1;32m  12581\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  12582\u001b[0m             \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/window/rolling.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, obj, window, min_periods, center, win_type, axis, on, closed, step, method, selection)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/window/rolling.py\u001b[0m in \u001b[0;36m_validate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1875\u001b[0m             \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_on\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mArrowDtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_on\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m\"mM\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1876\u001b[0m         ) and isinstance(self.window, (str, BaseOffset, timedelta)):\n\u001b[0;32m-> 1877\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_datetimelike_monotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m             \u001b[0;31m# this will raise ValueError on non-fixed freqs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/window/rolling.py\u001b[0m in \u001b[0;36m_validate_datetimelike_monotonic\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1922\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_monotonic_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"values must not have NaT\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_on\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_monotonic_increasing\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_on\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_monotonic_decreasing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1924\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_monotonic_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"values must be monotonic\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1926\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_raise_monotonic_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/window/rolling.py\u001b[0m in \u001b[0;36m_raise_monotonic_error\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m   1931\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1932\u001b[0m                 \u001b[0mon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"column\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1933\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{on} {msg}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1934\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1935\u001b[0m     @doc(\n",
            "\u001b[0;31mValueError\u001b[0m: index values must be monotonic"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 8 ‚Äì Stage 1 (LightGBM Binary Classifier, Categorical Columns, Early Stopping)"
      ],
      "metadata": {
        "id": "KRcH9Yce-8yC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8 ‚Äì Stage 1: LightGBM binary on ‚Äú> 15 min late‚Äù\n",
        "\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1) Reload the enriched data\n",
        "df = pd.read_pickle(AUGMENT_CACHE)\n",
        "df.columns = df.columns.str.upper()\n",
        "\n",
        "# 2) Guard any optional numeric columns\n",
        "for col in ['WIND_GUST_KPH','HOURLY_VOL','PRECIP_MM']:\n",
        "    if col not in df.columns:\n",
        "        df[col] = 0.0\n",
        "\n",
        "# 3) Define your feature set (including LATE_AIRCRAFT_DELAY)\n",
        "features = [\n",
        "    'DAY_OF_WEEK','CRS_DEP_MIN','DEP_HOUR_BIN','DEP_DELAY_NEW',\n",
        "    'DISTANCE_GROUP','TEMP_C','WIND_SPEED_KPH','WIND_GUST_KPH',\n",
        "    'PRECIP_MM','PRESSURE_HPA','HUMIDITY_PCT',\n",
        "    'PAST3_AVG_DELAY','PAST14_AVG_DELAY','ARR_DAY_PART',\n",
        "    'IS_HOLIDAY','PCT_ALATLATE_24H','SEASON','HOURLY_VOL',\n",
        "    'IS_SEVERE_WEATHER','LATE_AIRCRAFT_DELAY',\n",
        "    'OP_UNIQUE_CARRIER'\n",
        "]\n",
        "\n",
        "X = df[features].copy()\n",
        "# 4) New target: >15 min late\n",
        "y = (df['ARR_DELAY'] > 15).astype(int)\n",
        "\n",
        "# 5) Mark categorical columns\n",
        "cat_cols = ['DEP_HOUR_BIN','ARR_DAY_PART','OP_UNIQUE_CARRIER','SEASON']\n",
        "for c in cat_cols:\n",
        "    X[c] = X[c].astype('category')\n",
        "\n",
        "# 6) Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.20, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 7) Prepare LightGBM datasets\n",
        "lgb_tr = lgb.Dataset(X_train, label=y_train, categorical_feature=cat_cols, free_raw_data=False)\n",
        "lgb_ev = lgb.Dataset(X_test,  label=y_test,  reference=lgb_tr,  categorical_feature=cat_cols, free_raw_data=False)\n",
        "\n",
        "# 8) Train with early stopping (uses your best_params from before)\n",
        "clf = lgb.train(\n",
        "    best_params,\n",
        "    lgb_tr,\n",
        "    num_boost_round=200,\n",
        "    valid_sets=[lgb_tr, lgb_ev],\n",
        "    valid_names=['train','eval'],\n",
        "    callbacks=[lgb.early_stopping(stopping_rounds=30)]\n",
        ")\n",
        "\n",
        "# 9) Evaluate at 0.5 cutoff\n",
        "y_pred = (clf.predict(X_test, num_iteration=clf.best_iteration) > 0.5).astype(int)\n",
        "print(\"Stage 1 (>15 min late) accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "AzDH60DW_AwB",
        "outputId": "90e0d94d-6811-466d-a1ee-41d14e36e36b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"['PAST3_AVG_DELAY', 'PAST14_AVG_DELAY', 'ARR_DAY_PART', 'IS_HOLIDAY', 'PCT_ALATLATE_24H', 'SEASON', 'IS_SEVERE_WEATHER'] not in index\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-3832051554>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m ]\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;31m# 4) New target: >15 min late\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ARR_DELAY'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4107\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4108\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4110\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6198\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6200\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6202\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6251\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6252\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6254\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['PAST3_AVG_DELAY', 'PAST14_AVG_DELAY', 'ARR_DAY_PART', 'IS_HOLIDAY', 'PCT_ALATLATE_24H', 'SEASON', 'IS_SEVERE_WEATHER'] not in index\""
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 9: Confusion Matrix & Classification report"
      ],
      "metadata": {
        "id": "bI5YBbtiKpJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9 ‚Äì Confusion Matrix & Classification Report\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Re-compute predictions on your test set\n",
        "y_pred = (clf.predict(X_test, num_iteration=clf.best_iteration) >= best_thresh).astype(int)\n",
        "\n",
        "# 1) Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix (rows=true, cols=predicted):\")\n",
        "print(cm)\n",
        "\n",
        "# 2) Detailed classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(\n",
        "    y_test, y_pred,\n",
        "    target_names=['‚â§ 15 min late (on-time)','> 15 min late']\n",
        "))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9duqRESKv5J",
        "outputId": "c4117ccf-7f49-45ca-d7eb-b34fc9ba640a"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix (rows=true, cols=predicted):\n",
            "[[18195   317]\n",
            " [  954  2628]]\n",
            "\n",
            "Classification Report:\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "‚â§ 15 min late (on-time)       0.95      0.98      0.97     18512\n",
            "          > 15 min late       0.89      0.73      0.81      3582\n",
            "\n",
            "               accuracy                           0.94     22094\n",
            "              macro avg       0.92      0.86      0.89     22094\n",
            "           weighted avg       0.94      0.94      0.94     22094\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 10 - Stage 2: delay minutes regression on >15"
      ],
      "metadata": {
        "id": "zS3_BNMQRz2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10 ‚Äì Stage 2: delay‚Äêminutes regression on >15‚Äâmin late flights\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# 1) Reload augmented to pull actual ARR_DELAY\n",
        "augmented = pd.read_pickle(AUGMENT_CACHE)\n",
        "augmented.columns = augmented.columns.str.upper()\n",
        "\n",
        "# 2) Identify truly-late flights in train/test splits\n",
        "late_train_idx = y_train[y_train == 1].index\n",
        "late_test_idx  = y_test[y_test == 1].index\n",
        "\n",
        "X2_train = X_train.loc[late_train_idx].copy()\n",
        "y2_train = augmented.loc[late_train_idx, 'ARR_DELAY']\n",
        "\n",
        "X2_test  = X_test.loc[ late_test_idx].copy()\n",
        "y2_test  = augmented.loc[ late_test_idx, 'ARR_DELAY']\n",
        "\n",
        "print(f\"Training on {len(X2_train)} late‚Äêflight rows; testing on {len(X2_test)}\")\n",
        "\n",
        "# 3) One‚Äêhot encode categorical columns\n",
        "cat_cols = ['DEP_HOUR_BIN','ARR_DAY_PART','SEASON','OP_UNIQUE_CARRIER']\n",
        "X2_train_enc = pd.get_dummies(X2_train, columns=cat_cols, drop_first=True)\n",
        "X2_test_enc  = pd.get_dummies(X2_test,  columns=cat_cols, drop_first=True)\n",
        "\n",
        "# 4) Align train/test columns (fill any missing dummies with 0)\n",
        "X2_train_enc, X2_test_enc = X2_train_enc.align(\n",
        "    X2_test_enc, join='left', axis=1, fill_value=0\n",
        ")\n",
        "\n",
        "# 5) Train a RandomForestRegressor\n",
        "rf = RandomForestRegressor(n_estimators=200, max_depth=8, random_state=42)\n",
        "rf.fit(X2_train_enc, y2_train)\n",
        "\n",
        "# 6) Evaluate MAE on the test subset\n",
        "pred_delay = rf.predict(X2_test_enc)\n",
        "mae = mean_absolute_error(y2_test, pred_delay)\n",
        "print(\"Stage 2 MAE on truly late flights:\", round(mae, 2), \"minutes\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZRaYWO1R_Il",
        "outputId": "c5738b25-9bb6-4b65-9639-df01f42f9a33"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 14326 late‚Äêflight rows; testing on 3582\n",
            "Stage 2 MAE on truly late flights: 16.12 minutes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 11 Prob calibration"
      ],
      "metadata": {
        "id": "fg-l_EhhSAul"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Threshold tuning via Precision Recall"
      ],
      "metadata": {
        "id": "ucZEe8K-MF-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10 ‚Äì Threshold tuning via Precision-Recall curve\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "# 1) Get model probabilities:\n",
        "probs = clf.predict(X_test, num_iteration=clf.best_iteration)\n",
        "\n",
        "# 2) Compute precision, recall for many thresholds:\n",
        "prec, rec, thresh = precision_recall_curve(y_test, probs)\n",
        "\n",
        "# 3) Find threshold that gives ~90% recall:\n",
        "target_recall = 0.90\n",
        "idx = np.argmax(rec >= target_recall)\n",
        "chosen_thresh = thresh[idx]\n",
        "print(f\"At recall‚â•{target_recall:.2f}, threshold={chosen_thresh:.3f}, precision={prec[idx]:.3f}\")\n",
        "\n",
        "# 4) Re-compute confusion matrix at that new threshold:\n",
        "new_pred = (probs >= chosen_thresh).astype(int)\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(\"New Confusion Matrix:\", confusion_matrix(y_test, new_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5Ybbpn1MVEg",
        "outputId": "1f1531da-bd5e-49b4-ab78-01795b141199"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "At recall‚â•0.90, threshold=0.002, precision=0.162\n",
            "New Confusion Matrix: [[    0 18512]\n",
            " [    0  3582]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Find the F1"
      ],
      "metadata": {
        "id": "j6sZ5U_dNLle"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11 ‚Äì Pick threshold that maximizes F1\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_curve, f1_score, confusion_matrix\n",
        "\n",
        "# 1) Get model probabilities again\n",
        "probs = clf.predict(X_test, num_iteration=clf.best_iteration)\n",
        "\n",
        "# 2) Compute precision, recall, thresholds\n",
        "prec, rec, thresh = precision_recall_curve(y_test, probs)\n",
        "\n",
        "# 3) Compute F1 scores\n",
        "f1_scores = 2 * (prec * rec) / (prec + rec + 1e-12)\n",
        "\n",
        "# 4) Find best index\n",
        "best_idx = np.nanargmax(f1_scores)\n",
        "best_thresh = thresh[best_idx]\n",
        "best_f1    = f1_scores[best_idx]\n",
        "best_prec  = prec[best_idx]\n",
        "best_rec   = rec[best_idx]\n",
        "\n",
        "print(f\"Best F1 = {best_f1:.3f} at threshold = {best_thresh:.3f}\")\n",
        "print(f\"  Precision = {best_prec:.3f}, Recall = {best_rec:.3f}\")\n",
        "\n",
        "# 5) Show confusion matrix at that threshold\n",
        "y_pred_opt = (probs >= best_thresh).astype(int)\n",
        "cm_opt = confusion_matrix(y_test, y_pred_opt)\n",
        "print(\"\\nConfusion Matrix at F1-opt threshold:\")\n",
        "print(cm_opt)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HPq2QSMNQJk",
        "outputId": "dff16630-69e7-4725-8d4b-eafd39318980"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best F1 = 0.805 at threshold = 0.565\n",
            "  Precision = 0.892, Recall = 0.734\n",
            "\n",
            "Confusion Matrix at F1-opt threshold:\n",
            "[[18195   317]\n",
            " [  953  2629]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install CatBoost"
      ],
      "metadata": {
        "id": "kLsSdY5WDiLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9 ‚Äì Install CatBoost\n",
        "!pip install catboost --quiet\n"
      ],
      "metadata": {
        "id": "-M-RjmSBC7ul"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensemble of two LightGBM models (average predictions)"
      ],
      "metadata": {
        "id": "-3wejTXJ_Ze3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10 ‚Äì Stage 1: On-Time vs Late (CatBoostClassifier), with missing‚Äêcolumn guards\n",
        "\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1) Copy augmented and fill any missing numeric cols\n",
        "df = augmented.copy()\n",
        "for col in ['WIND_GUST_KPH', 'PRECIP_MM', 'PCT_ALATLATE_24H', 'HOURLY_VOL']:\n",
        "    if col not in df.columns:\n",
        "        df[col] = 0.0\n",
        "\n",
        "# 2) Define features & target\n",
        "features = [\n",
        "    'DAY_OF_WEEK','CRS_DEP_MIN','DEP_HOUR_BIN','DEP_DELAY_NEW',\n",
        "    'DISTANCE_GROUP','TEMP_C','WIND_SPEED_KPH','WIND_GUST_KPH',\n",
        "    'PRECIP_MM','PRESSURE_HPA','HUMIDITY_PCT',\n",
        "    'PAST3_AVG_DELAY','PAST14_AVG_DELAY','ARR_DAY_PART',\n",
        "    'IS_HOLIDAY','PCT_ALATLATE_24H','SEASON','HOURLY_VOL',\n",
        "    'OP_UNIQUE_CARRIER'\n",
        "]\n",
        "X = df[features].copy()\n",
        "y = (df['ARR_DELAY'] > 0).astype(int)\n",
        "\n",
        "# 3) Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.20, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 4) Mark categoricals\n",
        "cat_features = ['DEP_HOUR_BIN','ARR_DAY_PART','SEASON','OP_UNIQUE_CARRIER']\n",
        "\n",
        "# 5) Create Pools\n",
        "train_pool = Pool(data=X_train, label=y_train, cat_features=cat_features)\n",
        "eval_pool  = Pool(data=X_test,  label=y_test,  cat_features=cat_features)\n",
        "\n",
        "# 6) Train CatBoost\n",
        "print(\"\\n--- Stage 1: On-Time vs Late (CatBoost) ---\")\n",
        "cb = CatBoostClassifier(\n",
        "    iterations=200,\n",
        "    learning_rate=0.1,\n",
        "    depth=6,\n",
        "    eval_metric='Logloss',\n",
        "    random_seed=42,\n",
        "    early_stopping_rounds=30,\n",
        "    verbose=50\n",
        ")\n",
        "cb.fit(train_pool, eval_set=eval_pool)\n",
        "\n",
        "# 7) Evaluate\n",
        "pred_cb = cb.predict(eval_pool)\n",
        "print(\"CatBoost accuracy:\", accuracy_score(y_test, pred_cb))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5agjuC6_dze",
        "outputId": "e975e8ab-bdbe-401d-890c-caff7087a80c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Stage 1: On-Time vs Late (CatBoost) ---\n",
            "0:\tlearn: 0.6403852\ttest: 0.6405049\tbest: 0.6405049 (0)\ttotal: 273ms\tremaining: 54.3s\n",
            "50:\tlearn: 0.4164165\ttest: 0.4190752\tbest: 0.4190752 (50)\ttotal: 12s\tremaining: 35.1s\n",
            "100:\tlearn: 0.4052414\ttest: 0.4099957\tbest: 0.4099957 (100)\ttotal: 17s\tremaining: 16.7s\n",
            "150:\tlearn: 0.3976820\ttest: 0.4053949\tbest: 0.4053949 (150)\ttotal: 23.8s\tremaining: 7.72s\n",
            "199:\tlearn: 0.3918284\ttest: 0.4018949\tbest: 0.4018747 (198)\ttotal: 28.8s\tremaining: 0us\n",
            "\n",
            "bestTest = 0.4018746848\n",
            "bestIteration = 198\n",
            "\n",
            "Shrink model to first 199 iterations.\n",
            "CatBoost accuracy: 0.8287317823843577\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7d6757af"
      },
      "source": [
        "print(df.columns)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}